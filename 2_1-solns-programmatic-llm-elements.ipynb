{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8UpfZ5t4nQ6"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/2_1-solns-programmatic-llm-elements.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# Programmatic LLM Elements\n",
        "> For Vanderbilt University AI Summer 2024 Prepared by Dr. Charreau Bell\n",
        "\n",
        "_Code versions applicable: May 13, 2024_\n",
        "\n",
        "## Learning Outcomes:\n",
        "* Participants will be able to explain the core messaging elements of programmatic interaction with ChatLLMs, specifically system messages, user messages, and assistant messages.\n",
        "* Participants will be able to explain the programmatic requirements of conversational AI with completions-like APIs in relationship to context and chat history.\n",
        "* Participants will be able to integrate additional parameters of LLM API calls to modify the default behavior of the LLMs.\n",
        "* Participants will understand the usage of APIs within demonstrative applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n",
        "Here, we'll prepare the coding environment with packages and API keys. This notebook assumes Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0XrpXhflrB2k"
      },
      "outputs": [],
      "source": [
        "# install from pypi\n",
        "#! pip install openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# standard practice is all imports at the top, but for learning purposes, this is distributed throughout the notebook\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "agUgynklrpuG"
      },
      "outputs": [],
      "source": [
        "# make available to python\n",
        "#from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWyV7tAM4wKp"
      },
      "source": [
        "Here, you need to make sure that you have added your OpenAI API key:\n",
        "* Go to: https://platform.openai.com\n",
        "* Make sure that you are logged in\n",
        "* Go to sidebar API keys\n",
        "* Follow instructions\n",
        "* SAVE YOUR API KEY AS YOU WILL NEVER SEE IT WRITTEN AGAIN\n",
        "* Add it to the Colab sidebar with name `OPENAI_API_KEY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z-HGLB4_4-aN"
      },
      "outputs": [],
      "source": [
        "# set environment variable to be used by openAI client\n",
        "#os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Programmatic LLM API Elements\n",
        "Resources:\n",
        "* [OpenAI API Reference](https://platform.openai.com/docs/overview)\n",
        "* [Direct Link to Completions Quickstart](https://platform.openai.com/docs/quickstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get completion: From OpenAI Quickstart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "McZ09jJz1Zrf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletionMessage(content=\"In the realm of code, a concept profound,\\nRecursion dances, a loop so unsound.\\nLike a mirror reflecting its own reflection,\\nCalling itself, a self-referential collection.\\n\\nA function that calls itself, with purpose clear,\\nEach invocation solving a part of the query sphere.\\nDivide and conquer, the method it lends,\\nBreaking down problems until they amends.\\n\\nWith base cases to halt the endless chain,\\nRecursion weaves its magical, recursive refrain.\\nLike echoes bouncing in a cavern deep,\\nIt dives into solutions, promises to keep.\\n\\nFrom fractals to trees, Fibonacci's spiral,\\nRecursion's beauty makes programmers smile.\\nA looping dream, a method so bold,\\nInfinite depth, its story oft told.\\n\\nSo embrace recursion, let it guide your hand,\\nIn elegant loops, let your code expand.\\nFor in the world of programming's domain,\\nRecursion’s dance will surely remain.\", role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ],
      "source": [
        "# Copy from Quickstart\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9OD97mlPjM9HWGrEcbL48sf6FbPop',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': \"In the realm of code, a concept profound,\\nRecursion dances, a loop so unsound.\\nLike a mirror reflecting its own reflection,\\nCalling itself, a self-referential collection.\\n\\nA function that calls itself, with purpose clear,\\nEach invocation solving a part of the query sphere.\\nDivide and conquer, the method it lends,\\nBreaking down problems until they amends.\\n\\nWith base cases to halt the endless chain,\\nRecursion weaves its magical, recursive refrain.\\nLike echoes bouncing in a cavern deep,\\nIt dives into solutions, promises to keep.\\n\\nFrom fractals to trees, Fibonacci's spiral,\\nRecursion's beauty makes programmers smile.\\nA looping dream, a method so bold,\\nInfinite depth, its story oft told.\\n\\nSo embrace recursion, let it guide your hand,\\nIn elegant loops, let your code expand.\\nFor in the world of programming's domain,\\nRecursion’s dance will surely remain.\",\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1715557433,\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 184, 'prompt_tokens': 39, 'total_tokens': 223}}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# view structure of completion\n",
        "completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the realm of code, a concept profound,\n",
            "Recursion dances, a loop so unsound.\n",
            "Like a mirror reflecting its own reflection,\n",
            "Calling itself, a self-referential collection.\n",
            "\n",
            "A function that calls itself, with purpose clear,\n",
            "Each invocation solving a part of the query sphere.\n",
            "Divide and conquer, the method it lends,\n",
            "Breaking down problems until they amends.\n",
            "\n",
            "With base cases to halt the endless chain,\n",
            "Recursion weaves its magical, recursive refrain.\n",
            "Like echoes bouncing in a cavern deep,\n",
            "It dives into solutions, promises to keep.\n",
            "\n",
            "From fractals to trees, Fibonacci's spiral,\n",
            "Recursion's beauty makes programmers smile.\n",
            "A looping dream, a method so bold,\n",
            "Infinite depth, its story oft told.\n",
            "\n",
            "So embrace recursion, let it guide your hand,\n",
            "In elegant loops, let your code expand.\n",
            "For in the world of programming's domain,\n",
            "Recursion’s dance will surely remain.\n"
          ]
        }
      ],
      "source": [
        "# obtain the string content of the completion\n",
        "print(completion.choices[0].message.content)\n",
        "\n",
        "# save into variable just for convenience\n",
        "completion_1 = completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Updating the conversational context from response\n",
        "Tip: If developing locally, debuggers are your best friend!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GJ_LfnKY1c1F"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"},\n",
        "    # add updates to conversation here\n",
        "    {\"role\": \"assistant\", \"content\": completion_1},\n",
        "    {\"role\": \"user\", \"content\": \"Now explain python dictionaries.\"}\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9OD9BSttP1NtQfgBn0ieW1KFD43wU',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': \"In the realm of Python, where structures thrive,\\nDictionaries emerge, keeping data alive.\\nNot lists, not tuples, a different breed,\\nMapping keys to values, taking the lead.\\n\\nLike a real-world dictionary, but for code,\\nStoring information, on a digital road.\\nNo sequential order, keys hold the key,\\nAccessing values with lightning speed, you see.\\n\\nStrings, integers, even lists can play,\\nAs keys or as values, in a dictionary array.\\nImmutable keys, but values can change,\\nA dynamic duo, their powers exchange.\\n\\nLookup is swift, O(1) they say,\\nEfficient retrieval, no delay in the fray.\\nAdding, updating, deleting with ease,\\nDicts shine bright, like a code masterpiece.\\n\\nIterate through keys, through values, or both,\\nPython dicts offer flexibility and growth.\\nA powerhouse structure in Python's embrace,\\nDictionaries bring order, in a dynamic space.\\n\\nSo in Python's world, where beauty thrives,\\nDictionaries reign, where data survives.\\nA versatile tool, with power untold,\\nPython dictionaries, a treasure to behold.\",\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1715557437,\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 222,\n",
              "  'prompt_tokens': 236,\n",
              "  'total_tokens': 458}}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# view the response\n",
        "completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PNTnyNJk17UC",
        "outputId": "2de293e2-e477-4786-a944-d4cc50279a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the realm of Python, where structures thrive,\n",
            "Dictionaries emerge, keeping data alive.\n",
            "Not lists, not tuples, a different breed,\n",
            "Mapping keys to values, taking the lead.\n",
            "\n",
            "Like a real-world dictionary, but for code,\n",
            "Storing information, on a digital road.\n",
            "No sequential order, keys hold the key,\n",
            "Accessing values with lightning speed, you see.\n",
            "\n",
            "Strings, integers, even lists can play,\n",
            "As keys or as values, in a dictionary array.\n",
            "Immutable keys, but values can change,\n",
            "A dynamic duo, their powers exchange.\n",
            "\n",
            "Lookup is swift, O(1) they say,\n",
            "Efficient retrieval, no delay in the fray.\n",
            "Adding, updating, deleting with ease,\n",
            "Dicts shine bright, like a code masterpiece.\n",
            "\n",
            "Iterate through keys, through values, or both,\n",
            "Python dicts offer flexibility and growth.\n",
            "A powerhouse structure in Python's embrace,\n",
            "Dictionaries bring order, in a dynamic space.\n",
            "\n",
            "So in Python's world, where beauty thrives,\n",
            "Dictionaries reign, where data survives.\n",
            "A versatile tool, with power untold,\n",
            "Python dictionaries, a treasure to behold.\n"
          ]
        }
      ],
      "source": [
        "# just the text\n",
        "completion_2 = completion.choices[0].message.content\n",
        "print(completion_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### On your own: Continuing the conversation\n",
        "Now, continue the conversation based on the previous response. Your new question should be based on something about python dictionaries without specifically referencing it. Some examples:\n",
        "* \"Show me an example of a comprehension based on this.\"\n",
        "* \"How does this differ from a python list?\"\n",
        "\n",
        "View the response as a string, and not the total response object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfUAD7dn2KjW",
        "outputId": "17f72588-6b35-4948-eb28-9e25666e5e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In Python's realm, where comprehension gleams,\n",
            "Let's craft a dictionary, a coder's dreams.\n",
            "With keys as numbers, values as squares,\n",
            "Comprehension magic, beyond compare.\n",
            "\n",
            "```python\n",
            "# Using dictionary comprehension to create a dictionary of numbers and their squares\n",
            "squares_dict = {num: num**2 for num in range(1, 6)}\n",
            "print(squares_dict)\n",
            "```\n",
            "\n",
            "In this code snippet, the comprehension shines,\n",
            "Creating a dict, with numbers in lines.\n",
            "Range from 1 to 5, we loop, we compute,\n",
            "Squares of numbers, in a dictionary salute.\n",
            "\n",
            "Run this code, watch the magic unfold,\n",
            "A dictionary of squares, a story bold.\n",
            "Comprehension's power, concise and clear,\n",
            "In Python's kingdom, it's always near.\n"
          ]
        }
      ],
      "source": [
        "# Continue the conversation\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"},\n",
        "    {\"role\": \"assistant\", \"content\": completion_1},\n",
        "    {\"role\": \"user\", \"content\": \"Now explain python dictionaries.\"},\n",
        "    {\"role\": \"assistant\", \"content\": completion_2},\n",
        "    {\"role\": \"user\", \"content\": \"Show me an example of a comprehension based on this.\"},\n",
        "  ]\n",
        ")\n",
        "\n",
        "# view the response as a string\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## APIs: An exploration using OpenAI's Chat Completions API\n",
        "APIs are contracts between developers and services"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### APIs: Example 1 - Limiting Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9OD9I4TkrqBaE8Hm7Ke2jJ7ujdYFi',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': 'Once upon a time in a quaint little village nestled between rolling hills and lush forests, there lived a mischievous cat named Whiskers. Whiskers was a fluffy tabby with bright green eyes and a playful spirit that often got him into trouble. His favorite pastime was causing mischief and mayhem wherever he went, much to the exasperation of his loving owner, Mrs. Jenkins.\\n\\nNow, Mrs. Jenkins adored Whiskers with all her heart, but there was one task that always proved to be a challenge— trimming Whiskers\\' sharp claws. Every time she brought out the nail clippers, Whiskers would vanish into thin air, finding the most creative hiding spots in the house to avoid the dreaded nail-cutting session.\\n\\nOne day, as the sun dipped below the horizon and the soft glow of twilight filled the cozy cottage, Mrs. Jenkins decided it was time to tackle the task of trimming Whiskers\\' nails. She called out to him, \"Whiskers, it\\'s time for your nail trim!\" but there was no sign of the mischievous cat.\\n\\nMrs. Jenkins searched high and low, peeking under the bed, behind the curtains, and even inside the kitchen cupboards, but Whiskers was nowhere to be found. Just as she was about to give up, a soft mew came from the direction of the bookshelf.\\n\\nMrs. Jenkins approached the bookshelf and there, nestled between the dusty tomes, was Whiskers with a mischievous glint in his eye. He had squeezed himself into a tight space, his fluffy tail twitching with amusement at his clever hiding spot.\\n\\n\"Ah, there you are, you little rascal,\" Mrs. Jenkins chuckled as she carefully reached for Whiskers. But as soon as she got close, Whiskers darted out from his hiding place and scampered under the sofa, his soft paws patting against the wooden floor in his mad dash to escape the nail clippers.\\n\\nMrs. Jenkins couldn\\'t help but smile at Whiskers\\' antics, admiring his creativity in finding new hiding spots each time. She patiently waited as Whiskers playfully peeked out from under the sofa, his eyes sparkling with mischief.\\n\\nAfter a playful game of hide-and-seek, Whiskers finally relented and allowed Mrs. Jenkins to trim his nails. With a gentle touch and a few treats as a reward, Mrs. Jenkins managed to trim Whiskers\\' claws, much to his dismay.\\n\\nAs the night settled in and the stars twinkled in the sky, Mrs. Jenkins cuddled Whiskers in her arms, feeling grateful for her mischievous feline companion and the joy he brought into her life, even during nail-trimming adventures.\\n\\nAnd so, in their cozy cottage surrounded by love and laughter, Whiskers and Mrs. Jenkins shared many more playful moments, with Whiskers always finding new and creative hiding spots whenever nail-trimming time rolled around.',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1715557444,\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 610, 'prompt_tokens': 59, 'total_tokens': 669}}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a fantastic storyteller and are an expert in telling long, engaging, highly descriptive, imaginative stories.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a story about an extremely mischievous cat who finds creative places to hide whenever his mommy needs to cut his nails.\"}\n",
        "]\n",
        "\n",
        "full_token_completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=conversation_messages,\n",
        "  # add other parameters\n",
        "  max_tokens=None\n",
        ")\n",
        "\n",
        "full_token_completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'stop'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# show reason for ending\n",
        "full_token_completion.choices[0].finish_reason"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9OD9SLJUJdczak5WJoCigAhWdmixQ',\n",
              " 'choices': [{'finish_reason': 'length',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': 'Once upon a time, in a small cozy cottage nestled on the edge of a picturesque village, lived',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1715557454,\n",
              " 'model': 'gpt-3.5-turbo-0125',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 20, 'prompt_tokens': 59, 'total_tokens': 79}}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add in specific valuef or max_tokens parameter\n",
        "max_token_completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=conversation_messages,\n",
        "  # add other parameters\n",
        "  max_tokens=20\n",
        ")\n",
        "\n",
        "max_token_completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'length'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# show reason for ending\n",
        "max_token_completion.choices[0].finish_reason"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### APIs: Example 2 - Drilldown Inputs\n",
        "Sometimes you'll need to navigate deep into the API to get the information you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-9OD9V96bgcIUWt2GlqtOBAOo7KhQi',\n",
              " 'choices': [{'finish_reason': 'length',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': 'In a sunlit kitchen, amidst the chirping of the morning birds and the gentle breeze flowing through the open window, there reigned a magnificent large Maine Coon cat named Sir Whiskers. Known for his lush, golden brown fur and striking green eyes, Sir Whiskers had an air of nobility about him that was hard to overlook.\\n\\nOne particular day, Sir Whiskers was left alone as his human companions went about their daily errands. Known for his curious and mischie',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1715557457,\n",
              " 'model': 'gpt-4-turbo-2024-04-09',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': 'fp_0737e0dfd9',\n",
              " 'usage': {'completion_tokens': 100,\n",
              "  'prompt_tokens': 815,\n",
              "  'total_tokens': 915}}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Try completions with images\n",
        "image_response = client.chat.completions.create(\n",
        "    model=\"gpt-4-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a fantastic storyteller and are an expert in telling long, engaging, highly descriptive, imaginative stories.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"Tell me a story about the cat in this image based on what it is doing.\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": \"https://www.boltonvet.com/wp-content/uploads/2021/05/shutterstock_1929619196.jpg\"},\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=100,\n",
        ")\n",
        "\n",
        "image_response.model_dump()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Breakout Room: Chat Completions API (10 minutes)\n",
        "In this breakout room, you're going to explore implementing different parameters of the Chat Completions API. It has lots of functionality, and you can explore it based on your interests and those of your lab/group partners.\n",
        "\n",
        "In your breakout room:\n",
        "1. Choose a group leader to share your screen, and choose a writer who will document the results of the exploration.\n",
        "\n",
        "**[Creating Chat Completions](https://platform.openai.com/docs/api-reference/chat/create)**\n",
        "  * Read over all of the parameters that can be used in the request. Other than messages and model, which 3 parameters seem most interesting/useful for your purposes? Why?\n",
        "  * Choose a straightforward parameter of interest (e.g., temperature, seed, presence_penalty) and test it out with a simple prompt. What happens when you change the value of this parameter?\n",
        "  * [If time] Repeat the above with another straightforward parameter of interest.\n",
        "  * [Optional] Repeat the above with another parameter of interest, particularly if you are interested in `logprobs`, `response_format`, etc. We will work with tools another day.\n",
        "\n",
        "**[The chat completion object](https://platform.openai.com/docs/api-reference/chat/object)**\n",
        "  * What is the structure of choices? How would you access choice responses for n>1? Implement this below.\n",
        "  * What is the system fingerprint? In what cases do you think it would be useful?\n",
        "\n",
        "**[Extra Credit: The Moderation API](https://platform.openai.com/docs/api-reference/moderations)**\n",
        "  * What is the purpose of the moderation API? How might it be useful in a chatbot context?\n",
        "  * What models are available to do moderation? Do they appear to be GPT (LLM-based) models based on the models available and language of the API? Justify your answer and speculate in its implications/reasoning.\n",
        "  * Examine the example request given in the API documentation and try it out here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sample Code Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index-based access:\n",
            "Choice 0:\n",
            " In code's dance, recursive call,\n",
            "Function echoes its own song.\n",
            "A loop that dreams in circles small,\n",
            "Solving puzzles, endless throng. \n",
            "\n",
            "Choice 1:\n",
            " Infinite mirror,\n",
            "Calling itself over and\n",
            "over, recursion. \n",
            "\n",
            "Choice 2:\n",
            " Infinite loop spins,\n",
            "Function calling itself on,\n",
            "Recursion takes hold. \n",
            "\n",
            "\n",
            "Iterative approach:\n",
            "Choice: 0\n",
            "In code's dance, recursive call,\n",
            "Function echoes its own song.\n",
            "A loop that dreams in circles small,\n",
            "Solving puzzles, endless throng.\n",
            "\n",
            "Choice: 1\n",
            "Infinite mirror,\n",
            "Calling itself over and\n",
            "over, recursion.\n",
            "\n",
            "Choice: 2\n",
            "Infinite loop spins,\n",
            "Function calling itself on,\n",
            "Recursion takes hold.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# chat completions implementation 1, object 1\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts in short haiku.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
        "  ],\n",
        "  n=3,\n",
        "  max_tokens=30\n",
        ")\n",
        "\n",
        "# index access\n",
        "print('Index-based access:')\n",
        "print('Choice 0:\\n', completion.choices[0].message.content, '\\n')\n",
        "print('Choice 1:\\n', completion.choices[1].message.content, '\\n')\n",
        "print('Choice 2:\\n', completion.choices[2].message.content, '\\n')\n",
        "\n",
        "# iterative programmatic access\n",
        "print('\\nIterative approach:')\n",
        "\n",
        "# use langchain-style fstring formatting\n",
        "choice_template = f\"Choice: {{choice_index}}\\n{{choice_content}}\\n\"\n",
        "\n",
        "# iterate over choices\n",
        "for choice in completion.choices:\n",
        "    print(choice_template.format(choice_index = choice.index,\n",
        "                                 choice_content = choice.message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'modr-9OD9ZoWI6tbV2Jvj8MlSyzEPBnZYQ',\n",
              " 'model': 'text-moderation-007',\n",
              " 'results': [{'categories': {'harassment': True,\n",
              "    'harassment_threatening': True,\n",
              "    'hate': False,\n",
              "    'hate_threatening': False,\n",
              "    'self_harm': False,\n",
              "    'self_harm_instructions': False,\n",
              "    'self_harm_intent': False,\n",
              "    'sexual': False,\n",
              "    'sexual_minors': False,\n",
              "    'violence': True,\n",
              "    'violence_graphic': False,\n",
              "    'self-harm': False,\n",
              "    'sexual/minors': False,\n",
              "    'hate/threatening': False,\n",
              "    'violence/graphic': False,\n",
              "    'self-harm/intent': False,\n",
              "    'self-harm/instructions': False,\n",
              "    'harassment/threatening': True},\n",
              "   'category_scores': {'harassment': 0.5278584957122803,\n",
              "    'harassment_threatening': 0.5712487697601318,\n",
              "    'hate': 0.2324090600013733,\n",
              "    'hate_threatening': 0.024183575063943863,\n",
              "    'self_harm': 2.3696395601291442e-06,\n",
              "    'self_harm_instructions': 1.132860139030356e-09,\n",
              "    'self_harm_intent': 1.7161115692942985e-06,\n",
              "    'sexual': 1.205232911161147e-05,\n",
              "    'sexual_minors': 7.506431387582779e-08,\n",
              "    'violence': 0.997192919254303,\n",
              "    'violence_graphic': 3.399916022317484e-05,\n",
              "    'self-harm': 2.3696395601291442e-06,\n",
              "    'sexual/minors': 7.506431387582779e-08,\n",
              "    'hate/threatening': 0.024183575063943863,\n",
              "    'violence/graphic': 3.399916022317484e-05,\n",
              "    'self-harm/intent': 1.7161115692942985e-06,\n",
              "    'self-harm/instructions': 1.132860139030356e-09,\n",
              "    'harassment/threatening': 0.5712487697601318},\n",
              "   'flagged': True}]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# using moderations API\n",
        "moderation = client.moderations.create(input=\"I want to kill them.\")\n",
        "moderation.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>violence</th>\n",
              "      <td>True</td>\n",
              "      <td>9.971929e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>harassment_threatening</th>\n",
              "      <td>True</td>\n",
              "      <td>5.712488e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>harassment</th>\n",
              "      <td>True</td>\n",
              "      <td>5.278585e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hate</th>\n",
              "      <td>False</td>\n",
              "      <td>2.324091e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hate_threatening</th>\n",
              "      <td>False</td>\n",
              "      <td>2.418358e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>violence_graphic</th>\n",
              "      <td>False</td>\n",
              "      <td>3.399916e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sexual</th>\n",
              "      <td>False</td>\n",
              "      <td>1.205233e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>self_harm</th>\n",
              "      <td>False</td>\n",
              "      <td>2.369640e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>self_harm_intent</th>\n",
              "      <td>False</td>\n",
              "      <td>1.716112e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sexual_minors</th>\n",
              "      <td>False</td>\n",
              "      <td>7.506431e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>self_harm_instructions</th>\n",
              "      <td>False</td>\n",
              "      <td>1.132860e-09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        category         score\n",
              "violence                    True  9.971929e-01\n",
              "harassment_threatening      True  5.712488e-01\n",
              "harassment                  True  5.278585e-01\n",
              "hate                       False  2.324091e-01\n",
              "hate_threatening           False  2.418358e-02\n",
              "violence_graphic           False  3.399916e-05\n",
              "sexual                     False  1.205233e-05\n",
              "self_harm                  False  2.369640e-06\n",
              "self_harm_intent           False  1.716112e-06\n",
              "sexual_minors              False  7.506431e-08\n",
              "self_harm_instructions     False  1.132860e-09"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# moderations viewing helper\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'category':moderation.results[0].categories.model_dump(),\n",
        "                   'score':moderation.results[0].category_scores.model_dump()})\n",
        "df.index = df.index.str.replace('/', '_')\n",
        "df.drop_duplicates().sort_values(by=['category', 'score'], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Foundational behavior vs application behavior\n",
        "The code above is foundational programmatic access of LLMs (in OpenAI). Then, you add programming around it to actually make it useful.\n",
        "\n",
        "### \"Command line\" interaction\n",
        "Below is the functionality that keeps track of the history and makes the API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is an example of a function that can help to update chat history\n",
        "def get_assistant_response(user_message, llm_chat_history, update_chat_history=True, model_name = \"gpt-3.5-turbo\"):\n",
        "\n",
        "    ## completions as normal\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=llm_chat_history + [{'role': 'user', 'content': user_message}]\n",
        "    )\n",
        "\n",
        "    ## update chat history if desired\n",
        "    if update_chat_history:\n",
        "        new_messages = [{'role':'user', 'content': user_message},\n",
        "                        {'role': 'assistant', 'content': completion.choices[0].message.content}]\n",
        "        llm_chat_history.extend(new_messages)\n",
        "\n",
        "    ## return the response and updated chat history\n",
        "    return completion.choices[0].message.content, llm_chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below demonstrates our initialization and our \"user interface\". Best practice is to separate functionality from appearance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Begin chatting with the LLM!\n",
            "User:  Give me an example of a python dictionary\n",
            "Assistant: Sure! Here is an example of a Python dictionary:\n",
            "\n",
            "```python\n",
            "my_dict = {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\n",
            "```\n",
            "User:  Explain it\n",
            "Assistant: This Python dictionary named `my_dict` contains key-value pairs. Each key is associated with a value separated by a colon. In this example, we have keys such as \"name\", \"age\", and \"city\" with corresponding values \"Alice\", 30, and \"New York\".\n",
            "User:  Explain it like i'm 5 years old\n",
            "Assistant: A dictionary is like a big box where you can put different toys. In this box, we have toys labeled with names like \"name\", \"age\", and \"city\". Each toy has a special thing written on it. For example, the \"name\" toy has \"Alice\" written on it, the \"age\" toy has \"30\" written on it, and the \"city\" toy has \"New York\" written on it.\n"
          ]
        }
      ],
      "source": [
        "# create initial chat history\n",
        "system_message = 'You are a helpful assistant. Be brief, succinct, and clear in your responses. Only answer what is asked.'\n",
        "openai_chat_history = [{'role': 'system', 'content': system_message}]\n",
        "\n",
        "# wait for first input\n",
        "print('Begin chatting with the LLM!')\n",
        "user_input = input(\"You: \")\n",
        "\n",
        "# continue chatting until user types 'exit'\n",
        "while user_input != \"exit\":\n",
        "    print('User: ', user_input)\n",
        "    response, openai_chat_history = get_assistant_response(user_input, openai_chat_history)\n",
        "    print(\"Assistant:\", response)\n",
        "    user_input = input(\"You: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### UI Interaction\n",
        "We can also other programming elements (classes) to help us maintain the state, while helping us format the conversation for use in a streamlined library for developing UIs (gradio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "McgatKe3OLn3"
      },
      "outputs": [],
      "source": [
        "class OpenAIChatClient:\n",
        "    def __init__(self, model=\"gpt-3.5-turbo\", system_message=\"You are a helpful assistant.\"):\n",
        "        self.client = OpenAI() # assumes API key is in an environment\n",
        "        self.model = model\n",
        "\n",
        "        self.system_message = system_message\n",
        "        self.messages = [{\"role\": \"system\", \"content\": self.system_message}]  # Message list for OpenAI format\n",
        "        self.conversations = []  # Message list for gradio format\n",
        "        \n",
        "\n",
        "    def add_user_message(self, text):\n",
        "        self.messages.append({\"role\": \"user\", \"content\": text})\n",
        "        return self.call_completions_api()\n",
        "\n",
        "    def call_completions_api(self):\n",
        "\n",
        "      response = self.client.chat.completions.create(\n",
        "          model=self.model,\n",
        "          messages=self.messages\n",
        "      )\n",
        "\n",
        "      assistant_response = response.choices[0].message.content\n",
        "      self.messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "      user_message = self.messages[-2]['content']\n",
        "      self.conversations.append((user_message, assistant_response))\n",
        "\n",
        "      return assistant_response\n",
        "\n",
        "    def get_conversation(self):\n",
        "        return self.messages\n",
        "\n",
        "    def get_formatted_conversations(self):\n",
        "        return self.conversations\n",
        "    \n",
        "    def pretty_print_conversation(self):\n",
        "        return '\\n'.join([f\"{message['role'].capitalize()}: {message['content']}\" for message in self.messages])\n",
        "    \n",
        "    def reset_conversation(self, system_message = None):\n",
        "\n",
        "        # add a new system message if provided\n",
        "        if system_message:\n",
        "            self.system_message = system_message\n",
        "\n",
        "        # reset the messages and conversations\n",
        "        self.messages = [{\"role\": \"system\", \"content\": self.system_message}]\n",
        "        self.conversations = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now explore the behavior without the UI, and then with the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "z8jSKzjs8heq",
        "outputId": "d8ee45d2-9bb1-4b25-ab78-04ea86afdcb9"
      },
      "outputs": [],
      "source": [
        "# create OpenAI Client with history management\n",
        "custom_chat_client = OpenAIChatClient(system_message=system_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System: You are a helpful assistant. Be brief, succinct, and clear in your responses. Only answer what is asked.\n",
            "User: What is gradio?\n",
            "Assistant: Gradio is a Python library that allows you to quickly create UIs for your machine learning models and data visualizations.\n"
          ]
        }
      ],
      "source": [
        "# Simulate first interaction\n",
        "_ = custom_chat_client.add_user_message(\"What is gradio?\")\n",
        "\n",
        "# Do some formatting to make the printing prettier\n",
        "print(custom_chat_client.pretty_print_conversation())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omxU2NYV4MWs",
        "outputId": "44b3cb3d-390d-4013-ec8b-72bc9c756d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System: You are a helpful assistant. Be brief, succinct, and clear in your responses. Only answer what is asked.\n",
            "User: What is gradio?\n",
            "Assistant: Gradio is a Python library that allows you to quickly create UIs for your machine learning models and data visualizations.\n",
            "User: Create a minimal working user interface with the Blocks implementation (not Interface) in Python.\n",
            "Assistant: Here is a minimal example of creating a user interface using the Blocks implementation in Gradio:\n",
            "\n",
            "```python\n",
            "import gradio as gr\n",
            "\n",
            "def greet(name):\n",
            "    return \"Hello, \" + name + \"!\"\n",
            "\n",
            "iface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
            "iface.launch()\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Simulate first interaction\n",
        "_ = custom_chat_client.add_user_message(\"Create a minimal working user interface with the Blocks implementation (not Interface) in Python.\")\n",
        "\n",
        "# Do some formatting to make the printing prettier\n",
        "print(custom_chat_client.pretty_print_conversation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCcVC5I9808K"
      },
      "source": [
        "#### Integrate with gradio\n",
        "Gradio is another platform where the API is new and can be confusing to generative AI. Here, I usually just copy/paste/adapt what I need from the API reference.\n",
        "\n",
        "[Example ChatBot](https://www.gradio.app/guides/creating-a-chatbot-fast)\n",
        "\n",
        "Basic story:\n",
        "* Wherever you see `chatbot_history`, that basically represents message history.\n",
        "* Message history is formatted as a list of `(user_message, bot_message)` tuples. Hence, why we wanted the format above added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KFZTGMse88V5"
      },
      "outputs": [],
      "source": [
        "# reset conversation\n",
        "custom_chat_client.reset_conversation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "cOLRx6L7RWlR",
        "outputId": "44a2714d-3998-41b3-ff37-be1364df302e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMPORTANT: You are using gradio version 4.19.2, however version 4.29.0 is available, please upgrade.\n",
            "--------\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def respond(message, chat_history):\n",
        "  custom_chat_client.add_user_message(message)\n",
        "  return '', custom_chat_client.get_formatted_conversations()\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot_history = gr.Chatbot()\n",
        "    msg_textbox = gr.Textbox()\n",
        "    reset_button = gr.ClearButton([msg_textbox, chatbot_history]) #doesn't do anything right now\n",
        "\n",
        "    msg_textbox.submit(respond, inputs=[msg_textbox, chatbot_history], outputs=[msg_textbox, chatbot_history])\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue9lZGMFDUVp"
      },
      "source": [
        "# Homework\n",
        "Congratulations! You've made it through an incredible firehose introduction to implementing LLMs programmatically! For homework, you're tasked with gaining more depth into several of the concepts.\n",
        "\n",
        "## Required Exercises\n",
        "### Learning more about LLM Platforms\n",
        "Read and summarize the following short articles to gain more background on the OpenAI API:\n",
        "* [OpenAI API Documentation Introduction](https://platform.openai.com/docs/introduction)\n",
        "* [Text Generation](https://platform.openai.com/docs/guides/text-generation) (Note: You can skip `Completions API (Legacy)`)\n",
        "* [Moderation](https://platform.openai.com/docs/guides/moderation/overview)\n",
        "\n",
        "### Practice with the OpenAI API using Prompt Engineering\n",
        "* Read this short section on [Prompt Engineering by AWS](https://catalog.us-east-1.prod.workshops.aws/workshops/a4bdb007-5600-4368-81c5-ff5b4154f518/en-US/050-prompt-engineering), focusing **ONLY** on the prompt patterns introduced.\n",
        "* Use the OpenAI API to implement each of the patterns. Hint: This will look like the `client.chat.completions.create` cells that we have above, but with modified user messages.\n",
        "* How might some of these patterns benefit you in your project efforts?\n",
        "\n",
        "### Testing Your Understanding of LLM Concepts using a different ChatLLM API\n",
        "#### Implement the 3-turn conversation with Google's Gemini API\n",
        "Now, you will really test your mettle by implementing the 3-turn conversation at the beginning of this notebook using a completely different API - Google's Gemini API. You want to make sure to implement it as \"multi-turn\"; in other words, that you are making sure that the chat history is sent in the full context somehow. There will be differences in this API from the OpenAI implementation. Based on all of the steps above, starting from authentication, implement interaction with the prompts above. Below are some hints/steps to help if needed.\n",
        "\n",
        "<details>\n",
        "<summary>  Show Hints </summary>\n",
        "<div style=\"margin-left: 20px;\">\n",
        "    <details>\n",
        "        <summary>Step 1</summary>\n",
        "        <p>Locate the Gemini API. You can find the platform overview <a href=https://ai.google.dev/>here</a>, or the API documentation <a href=https://ai.google.dev/gemini-api/docs>here</a></p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 2</summary>\n",
        "        <p><strong> Create your API key.</strong> Read the <a href=https://ai.google.dev/gemini-api/docs/quickstart>Quick Start</a>. Note that this Quick Start also has a button where you can \"Run in Google Colab\". That's a great place to start. You can either follow the instructions in the Quick Start, or open the Colab notebook and follow the instructions from there.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 3</summary>\n",
        "        <p> <strong> Setup your Colab environment with API keys. </strong> If you haven't already, open up the Colab notebook in the <a href=https://ai.google.dev/gemini-api/docs/quickstart>Quick Start</a> or create your own Colab notebook and copying [while understanding] the cells). Don't forget to use the Colab key in the sidebar. That's where you can set your API key.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 4</summary>\n",
        "        <p> <strong> Read the API overview and implement the steps, paying particular attention to the multi-turn conversation. </strong> Sidebars are your friend. Find the API Overview and begin reading to learn more and figure out what you need to do.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 5</summary>\n",
        "        <p> Once you understand what you're doing, write/adapt the code so that it does what you expect. Verify the behavior.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Hint</summary>\n",
        "        <p> If all fails, you always have chatGPT/Gemini/ChatLLMofChoice to help you! You have the code on the Gemini API page - use it in your context and ask questions about it to help you come to the answer.</p>\n",
        "    </details>\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "#### Explore Differences\n",
        "Now that you've implemented the functionality, describe the similarities/differences that you see in:\n",
        "* API Key usage\n",
        "* API implementation code structure (e.g., OpenAI used `client.chat.completions.create` - what about Google?)\n",
        "* Model selection\n",
        "* Usage of role-based prompts (i.e., \"user\", \"system\", etc)\n",
        "* Implementation of chat history\n",
        "* Overall programmatic feel\n",
        "* Anything else you observe\n",
        "\n",
        "## Optional Exercises\n",
        "1. We briefly went over applications/interaction using the OpenAI API. Use ChatGPT/Colab AI/ChatLLMofChoice to make sure that you understand the code.\n",
        "2. Ignore the class and function that were created for interaction. Use generative AI (or not) to help you create your own code to maintain chat history when needed.\n",
        "3. Read more about [Gradio](https://www.gradio.app/) (you know what to do! Quickstart -> API Docs) and add other components to the user interface. You can start by using the `reset_button` and adding functionality to reset the chat LLM so it is just the default system message."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
