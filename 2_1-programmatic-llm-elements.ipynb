{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8UpfZ5t4nQ6"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/2_1-programmatic-llm-elements.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# Programmatic LLM Elements\n",
        "> For Vanderbilt University AI Summer 2024 Prepared by Dr. Charreau Bell\n",
        "\n",
        "_Code versions applicable: May 13, 2024_\n",
        "\n",
        "## Learning Outcomes:\n",
        "* Participants will be able to explain the core messaging elements of programmatic interaction with ChatLLMs, specifically system messages, user messages, and assistant messages.\n",
        "* Participants will be able to explain the programmatic requirements of conversational AI with completions-like APIs in relationship to context and chat history.\n",
        "* Participants will be able to integrate additional parameters of LLM API calls to modify the default behavior of the LLMs.\n",
        "* Participants will understand the usage of APIs within demonstrative applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n",
        "Here, we'll prepare the coding environment with packages and API keys. This notebook assumes Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XrpXhflrB2k"
      },
      "outputs": [],
      "source": [
        "# install from pypi\n",
        "! pip install openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# standard practice is all imports at the top, but for learning purposes, this is distributed throughout the notebook\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agUgynklrpuG"
      },
      "outputs": [],
      "source": [
        "# make available to python\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWyV7tAM4wKp"
      },
      "source": [
        "Here, you need to make sure that you have added your OpenAI API key:\n",
        "* Go to: https://platform.openai.com\n",
        "* Make sure that you are logged in\n",
        "* Go to sidebar API keys\n",
        "* Follow instructions\n",
        "* SAVE YOUR API KEY AS YOU WILL NEVER SEE IT WRITTEN AGAIN\n",
        "* Add it to the Colab sidebar with name `OPENAI_API_KEY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-HGLB4_4-aN"
      },
      "outputs": [],
      "source": [
        "# set environment variable to be used by openAI client\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Programmatic LLM API Elements\n",
        "Resources:\n",
        "* [OpenAI API Reference](https://platform.openai.com/docs/overview)\n",
        "* [Direct Link to Completions Quickstart](https://platform.openai.com/docs/quickstart)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get completion: From OpenAI Quickstart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McZ09jJz1Zrf"
      },
      "outputs": [],
      "source": [
        "# Copy from Quickstart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# view structure of completion\n",
        "completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obtain the string content of the completion\n",
        "print(completion.choices[0].message.content)\n",
        "\n",
        "# save into variable just for convenience\n",
        "completion_1 = completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Updating the conversational context from response\n",
        "Tip: If developing locally, debuggers are your best friend!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ_LfnKY1c1F"
      },
      "outputs": [],
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
        "    # add updates to conversation here\n",
        "\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# view the response\n",
        "completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PNTnyNJk17UC",
        "outputId": "2de293e2-e477-4786-a944-d4cc50279a92"
      },
      "outputs": [],
      "source": [
        "# just the text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### On your own: Continuing the conversation\n",
        "Now, continue the conversation based on the previous response. Your new question should be based on something about python dictionaries without specifically referencing it. Some examples:\n",
        "* \"Show me an example of a comprehension based on this.\"\n",
        "* \"How does this differ from a python list?\"\n",
        "\n",
        "View the response as a string, and not the total response object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfUAD7dn2KjW",
        "outputId": "17f72588-6b35-4948-eb28-9e25666e5e01"
      },
      "outputs": [],
      "source": [
        "# Continue the conversation\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"},\n",
        "    {\"role\": \"assistant\", \"content\": completion_1},\n",
        "    {\"role\": \"user\", \"content\": \"Now explain python dictionaries.\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "# view the response as a string\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## APIs: An exploration using OpenAI's Chat Completions API\n",
        "APIs are contracts between developers and services"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### APIs: Example 1 - Limiting Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a fantastic storyteller and are an expert in telling long, engaging, highly descriptive, imaginative stories.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a story about an extremely mischievous cat who finds creative places to hide whenever his mommy needs to cut his nails.\"}\n",
        "]\n",
        "\n",
        "full_token_completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=conversation_messages,\n",
        "  # add other parameters\n",
        "  \n",
        ")\n",
        "\n",
        "full_token_completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show reason for ending\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add in specific valuef or max_tokens parameter\n",
        "max_token_completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=conversation_messages,\n",
        "  # add other parameters\n",
        "\n",
        ")\n",
        "\n",
        "max_token_completion.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show reason for ending\n",
        "max_token_completion.choices[0].finish_reason"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### APIs: Example 2 - Drilldown Inputs\n",
        "Sometimes you'll need to navigate deep into the API to get the information you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try completions with images\n",
        "\n",
        "\n",
        "image_response.model_dump()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Breakout Room: Chat Completions API (10 minutes)\n",
        "In this breakout room, you're going to explore implementing different parameters of the Chat Completions API. It has lots of functionality, and you can explore it based on your interests and those of your lab/group partners.\n",
        "\n",
        "In your breakout room:\n",
        "1. Choose a group leader to share your screen, and choose a writer who will document the results of the exploration.\n",
        "\n",
        "**[Creating Chat Completions](https://platform.openai.com/docs/api-reference/chat/create)**\n",
        "  * Read over all of the parameters that can be used in the request. Other than messages and model, which 3 parameters seem most interesting/useful for your purposes? Why?\n",
        "  * Choose a straightforward parameter of interest (e.g., temperature, seed, presence_penalty) and test it out with a simple prompt. What happens when you change the value of this parameter?\n",
        "  * [If time] Repeat the above with another straightforward parameter of interest.\n",
        "  * [Optional] Repeat the above with another parameter of interest, particularly if you are interested in `logprobs`, `response_format`, etc. We will work with tools another day.\n",
        "\n",
        "**[The chat completion object](https://platform.openai.com/docs/api-reference/chat/object)**\n",
        "  * What is the structure of choices? How would you access choice responses for n>1? Implement this below.\n",
        "  * What is the system fingerprint? In what cases do you think it would be useful?\n",
        "\n",
        "**[Extra Credit: The Moderation API](https://platform.openai.com/docs/api-reference/moderations)**\n",
        "  * What is the purpose of the moderation API? How might it be useful in a chatbot context?\n",
        "  * What models are available to do moderation? Do they appear to be GPT (LLM-based) models based on the models available and language of the API? Justify your answer and speculate in its implications/reasoning.\n",
        "  * Examine the example request given in the API documentation and try it out here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sample Code Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# chat completions implementation 1, object 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# using moderations API (call your variable `moderation` and the code below will work)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# moderations viewing helper\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'category':moderation.results[0].categories.model_dump(),\n",
        "                   'score':moderation.results[0].category_scores.model_dump()})\n",
        "df.index = df.index.str.replace('/', '_')\n",
        "df.drop_duplicates().sort_values(by=['category', 'score'], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Foundational behavior vs application behavior\n",
        "The code above is foundational programmatic access of LLMs (in OpenAI). Then, you add programming around it to actually make it useful.\n",
        "\n",
        "### \"Command line\" interaction\n",
        "Below is the functionality that keeps track of the history and makes the API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is an example of a function that can help to update chat history\n",
        "def get_assistant_response(user_message, llm_chat_history, update_chat_history=True, model_name = \"gpt-3.5-turbo\"):\n",
        "\n",
        "    ## completions as normal\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=llm_chat_history + [{'role': 'user', 'content': user_message}]\n",
        "    )\n",
        "\n",
        "    ## update chat history if desired\n",
        "    if update_chat_history:\n",
        "        new_messages = [{'role':'user', 'content': user_message},\n",
        "                        {'role': 'assistant', 'content': completion.choices[0].message.content}]\n",
        "        llm_chat_history.extend(new_messages)\n",
        "\n",
        "    ## return the response and updated chat history\n",
        "    return completion.choices[0].message.content, llm_chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below demonstrates our initialization and our \"user interface\". Best practice is to separate functionality from appearance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create initial chat history\n",
        "system_message = 'You are a helpful assistant. Be brief, succinct, and clear in your responses. Only answer what is asked.'\n",
        "openai_chat_history = [{'role': 'system', 'content': system_message}]\n",
        "\n",
        "# wait for first input\n",
        "print('Begin chatting with the LLM!')\n",
        "user_input = input(\"You: \")\n",
        "\n",
        "# continue chatting until user types 'exit'\n",
        "while user_input != \"exit\":\n",
        "    print('User: ', user_input)\n",
        "    response, openai_chat_history = get_assistant_response(user_input, openai_chat_history)\n",
        "    print(\"Assistant:\", response)\n",
        "    user_input = input(\"You: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### UI Interaction\n",
        "We can also other programming elements (classes) to help us maintain the state, while helping us format the conversation for use in a streamlined library for developing UIs (gradio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McgatKe3OLn3"
      },
      "outputs": [],
      "source": [
        "class OpenAIChatClient:\n",
        "    def __init__(self, model=\"gpt-3.5-turbo\", system_message=\"You are a helpful assistant.\"):\n",
        "        self.client = OpenAI() # assumes API key is in an environment\n",
        "        self.model = model\n",
        "\n",
        "        self.system_message = system_message\n",
        "        self.messages = [{\"role\": \"system\", \"content\": self.system_message}]  # Message list for OpenAI format\n",
        "        self.conversations = []  # Message list for gradio format\n",
        "        \n",
        "\n",
        "    def add_user_message(self, text):\n",
        "        self.messages.append({\"role\": \"user\", \"content\": text})\n",
        "        return self.call_completions_api()\n",
        "\n",
        "    def call_completions_api(self):\n",
        "\n",
        "      response = self.client.chat.completions.create(\n",
        "          model=self.model,\n",
        "          messages=self.messages\n",
        "      )\n",
        "\n",
        "      assistant_response = response.choices[0].message.content\n",
        "      self.messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "      user_message = self.messages[-2]['content']\n",
        "      self.conversations.append((user_message, assistant_response))\n",
        "\n",
        "      return assistant_response\n",
        "\n",
        "    def get_conversation(self):\n",
        "        return self.messages\n",
        "\n",
        "    def get_formatted_conversations(self):\n",
        "        return self.conversations\n",
        "    \n",
        "    def pretty_print_conversation(self):\n",
        "        return '\\n'.join([f\"{message['role'].capitalize()}: {message['content']}\" for message in self.messages])\n",
        "    \n",
        "    def reset_conversation(self, system_message = None):\n",
        "\n",
        "        # add a new system message if provided\n",
        "        if system_message:\n",
        "            self.system_message = system_message\n",
        "\n",
        "        # reset the messages and conversations\n",
        "        self.messages = [{\"role\": \"system\", \"content\": self.system_message}]\n",
        "        self.conversations = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now explore the behavior without the UI, and then with the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "z8jSKzjs8heq",
        "outputId": "d8ee45d2-9bb1-4b25-ab78-04ea86afdcb9"
      },
      "outputs": [],
      "source": [
        "# create OpenAI Client with history management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate first interaction\n",
        "\n",
        "\n",
        "# Do some formatting to make the printing prettier\n",
        "print(custom_chat_client.pretty_print_conversation())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omxU2NYV4MWs",
        "outputId": "44b3cb3d-390d-4013-ec8b-72bc9c756d79"
      },
      "outputs": [],
      "source": [
        "# Simulate first interaction\n",
        "_ = custom_chat_client.add_user_message(\"Create a minimal working user interface with the Blocks implementation (not Interface) in Python.\")\n",
        "\n",
        "# Do some formatting to make the printing prettier\n",
        "print(custom_chat_client.pretty_print_conversation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCcVC5I9808K"
      },
      "source": [
        "#### Integrate with gradio\n",
        "Gradio is another platform where the API is new and can be confusing to generative AI. Here, I usually just copy/paste/adapt what I need from the API reference.\n",
        "\n",
        "[Example ChatBot](https://www.gradio.app/guides/creating-a-chatbot-fast)\n",
        "\n",
        "Basic story:\n",
        "* Wherever you see `chatbot_history`, that basically represents message history.\n",
        "* Message history is formatted as a list of `(user_message, bot_message)` tuples. Hence, why we wanted the format above added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFZTGMse88V5"
      },
      "outputs": [],
      "source": [
        "# reset conversation\n",
        "custom_chat_client.reset_conversation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "cOLRx6L7RWlR",
        "outputId": "44a2714d-3998-41b3-ff37-be1364df302e"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def respond(message, chat_history):\n",
        "  custom_chat_client.add_user_message(message)\n",
        "  return '', custom_chat_client.get_formatted_conversations()\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot_history = gr.Chatbot()\n",
        "    msg_textbox = gr.Textbox()\n",
        "    reset_button = gr.ClearButton([msg_textbox, chatbot_history]) #doesn't do anything right now\n",
        "\n",
        "    msg_textbox.submit(respond, inputs=[msg_textbox, chatbot_history], outputs=[msg_textbox, chatbot_history])\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue9lZGMFDUVp"
      },
      "source": [
        "# Homework\n",
        "Congratulations! You've made it through an incredible firehose introduction to implementing LLMs programmatically! For homework, you're tasked with gaining more depth into several of the concepts.\n",
        "\n",
        "## Required Exercises\n",
        "### Learning more about LLM Platforms\n",
        "Read and summarize the following short articles to gain more background on the OpenAI API:\n",
        "* [OpenAI API Documentation Introduction](https://platform.openai.com/docs/introduction)\n",
        "* [Text Generation](https://platform.openai.com/docs/guides/text-generation) (Note: You can skip `Completions API (Legacy)`)\n",
        "* [Moderation](https://platform.openai.com/docs/guides/moderation/overview)\n",
        "\n",
        "### Practice with the OpenAI API using Prompt Engineering\n",
        "* Read this short section on [Prompt Engineering by AWS](https://catalog.us-east-1.prod.workshops.aws/workshops/a4bdb007-5600-4368-81c5-ff5b4154f518/en-US/050-prompt-engineering), focusing **ONLY** on the prompt patterns introduced.\n",
        "* Use the OpenAI API to implement each of the patterns. Hint: This will look like the `client.chat.completions.create` cells that we have above, but with modified user messages.\n",
        "* How might some of these patterns benefit you in your project efforts?\n",
        "\n",
        "### Testing Your Understanding of LLM Concepts using a different ChatLLM API\n",
        "#### Implement the 3-turn conversation with Google's Gemini API\n",
        "Now, you will really test your mettle by implementing the 3-turn conversation at the beginning of this notebook using a completely different API - Google's Gemini API. You want to make sure to implement it as \"multi-turn\"; in other words, that you are making sure that the chat history is sent in the full context somehow. There will be differences in this API from the OpenAI implementation. Based on all of the steps above, starting from authentication, implement interaction with the prompts above. Below are some hints/steps to help if needed.\n",
        "\n",
        "<details>\n",
        "<summary>  Show Hints </summary>\n",
        "<div style=\"margin-left: 20px;\">\n",
        "    <details>\n",
        "        <summary>Step 1</summary>\n",
        "        <p>Locate the Gemini API. You can find the platform overview <a href=https://ai.google.dev/>here</a>, or the API documentation <a href=https://ai.google.dev/gemini-api/docs>here</a></p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 2</summary>\n",
        "        <p><strong> Create your API key.</strong> Read the <a href=https://ai.google.dev/gemini-api/docs/quickstart>Quick Start</a>. Note that this Quick Start also has a button where you can \"Run in Google Colab\". That's a great place to start. You can either follow the instructions in the Quick Start, or open the Colab notebook and follow the instructions from there.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 3</summary>\n",
        "        <p> <strong> Setup your Colab environment with API keys. </strong> If you haven't already, open up the Colab notebook in the <a href=https://ai.google.dev/gemini-api/docs/quickstart>Quick Start</a> or create your own Colab notebook and copying [while understanding] the cells). Don't forget to use the Colab key in the sidebar. That's where you can set your API key.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 4</summary>\n",
        "        <p> <strong> Read the API overview and implement the steps, paying particular attention to the multi-turn conversation. </strong> Sidebars are your friend. Find the API Overview and begin reading to learn more and figure out what you need to do.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Step 5</summary>\n",
        "        <p> Once you understand what you're doing, write/adapt the code so that it does what you expect. Verify the behavior.</p>\n",
        "    </details>\n",
        "    <details>\n",
        "        <summary>Hint</summary>\n",
        "        <p> If all fails, you always have chatGPT/Gemini/ChatLLMofChoice to help you! You have the code on the Gemini API page - use it in your context and ask questions about it to help you come to the answer.</p>\n",
        "    </details>\n",
        "</div>\n",
        "</details>\n",
        "\n",
        "#### Explore Differences\n",
        "Now that you've implemented the functionality, describe the similarities/differences that you see in:\n",
        "* API Key usage\n",
        "* API implementation code structure (e.g., OpenAI used `client.chat.completions.create` - what about Google?)\n",
        "* Model selection\n",
        "* Usage of role-based prompts (i.e., \"user\", \"system\", etc)\n",
        "* Implementation of chat history\n",
        "* Overall programmatic feel\n",
        "* Anything else you observe\n",
        "\n",
        "## Optional Exercises\n",
        "1. We briefly went over applications/interaction using the OpenAI API. Use ChatGPT/Colab AI/ChatLLMofChoice to make sure that you understand the code.\n",
        "2. Ignore the class and function that were created for interaction. Use generative AI (or not) to help you create your own code to maintain chat history when needed.\n",
        "3. Read more about [Gradio](https://www.gradio.app/) (you know what to do! Quickstart -> API Docs) and add other components to the user interface. You can start by using the `reset_button` and adding functionality to reset the chat LLM so it is just the default system message."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
