{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/2_3-functions-tools-agents.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Tools, Agents, and Assistants\n",
    "> For Vanderbilt University AI Summer 2024<br>Prepared by Dr. Charreau Bell\n",
    "\n",
    "_Code versions applicable: May 17, 2024_\n",
    "\n",
    "## Learning Outcomes\n",
    "* Participants will understand schemas and algorithms for usage in function calling and tool calling applications and articulate the behavior.\n",
    "* Participants will be able to describe the behavior of agents and articulate fundamental advantages and disadvantages this approach.\n",
    "* Participants will be able to leverage schemas, tool calling, function calling, agents, API assistants to enhance the capabilities and flexibility of their generative AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain==0.1.20 langchain_openai sentence-transformers duckduckgo-search gradio\n",
    "! pip install pypdf chromadb faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth replicated here for reference just in case you choose to do something similar\n",
    "from google.colab import userdata\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable tracing and set project name\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"false\"\n",
    "\n",
    "# uncomment the following two lines before running the cell if you have a Langchain/Langsmith API Key\n",
    "#os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
    "#os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "\n",
    "# set langchain project\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'May17'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas\n",
    "Resources:\n",
    "* [Structured Output](https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Any\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example interview information\n",
    "interview_text = \"\"\"My name is Jackson. I'm from Pittsburgh, Pennsylvania, and I graduated college in 2019.\n",
    "I'm currently working as an administrative assistant at a law firm. I like to go hiking and play video games in my free time.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the output schema\n",
    "class PersonInformation(BaseModel):\n",
    "    # add docstring\n",
    "\n",
    "    # add fields\n",
    "    graduation_year: int = Field(default=None, description=\"The year the person graduated.\")\n",
    "    job_title: str = Field(default='', description=\"The person's job title.\")\n",
    "    hobbies: List[str] = Field(default=[], description=\"The person's hobbies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a model with function calling enabled\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_chatllm = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create human prompt to send\n",
    "text_prompt = f\"Extract the person's name, home, graduation year, job title, and hobbies from the following interview text: \\n\\n{interview_text}\"\n",
    "\n",
    "# get structured output (hopefully)\n",
    "structured_info = structured_chatllm.invoke(text_prompt)\n",
    "structured_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_info.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n",
    "## Function Calling\n",
    "Resource: [Tool/Function Calling](https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somewhat optional for this case\n",
    "class MultiplySchema(BaseModel):\n",
    "    \"\"\"Multiply two integer values together. These values must be integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer value\")\n",
    "    b: int = Field(..., description=\"Second integer value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define available tools\n",
    "\n",
    "tool_lookup = {'multiply': multiply}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "chatllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# binds the tools\n",
    "tool_chatllm = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use\n",
    "tool_response = tool_chatllm.invoke(\"What is 6 times 5?\")\n",
    "\n",
    "# view response\n",
    "tool_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get function that is to be called\n",
    "fn_to_call = #\n",
    "\n",
    "# call function\n",
    "tool_lookup[fn_to_call].invoke(# add invocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting into the function calling framework\n",
    "Resource: [Langchain v2 Tool Calling](https://python.langchain.com/v0.2/docs/how_to/tool_calling/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I have 6 books of 5 pages each. How many pages do I have in total?\"\n",
    "available_tools = {'multiply':multiply}\n",
    "\n",
    "# get response from llm\n",
    "llm_response = tool_chatllm.invoke(query)\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct message history to complete the rest of the AI Message response\n",
    "message_history = [HumanMessage(query), llm_response]\n",
    "\n",
    "# make all tool calls\n",
    "for tool_call in llm_response.tool_calls:\n",
    "    # get the name of the called tool from the response\n",
    "    called_tool_name = tool_call[\"name\"]\n",
    "\n",
    "    # get the tool object from the available tools\n",
    "    selected_tool = available_tools[called_tool_name]\n",
    "\n",
    "    # actually use the tool using the provided arguments\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "\n",
    "    # append to the message history\n",
    "    \n",
    "\n",
    "# view the message history    \n",
    "message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish the call to the AI\n",
    "final_answer = tool_chatllm.invoke(message_history)\n",
    "final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents\n",
    "Learn more about [Agents](https://python.langchain.com/docs/modules/agents/quick_start) in their Quickstart\n",
    "\n",
    "**Chain Tool Invocation**\n",
    "<figure>\n",
    "<img src='https://python.langchain.com/v0.1/assets/images/tool_chain-3571e7fbc481d648aff93a2630f812ab.svg' height=300/>\n",
    "    <figcaption>\n",
    "        Source: Chain Tool Invocation, from <a href=https://python.langchain.com/v0.1/docs/use_cases/tool_use/>Calling tools with Chains</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "**Agent-based Tool Invocation**\n",
    "<figure>\n",
    "<img src='https://python.langchain.com/v0.1/assets/images/tool_agent-d25fafc271da3ee950ac1fba59cdf490.svg' height=400/>\n",
    "    <figcaption>\n",
    "        Source: Agent-based Tool Invocation, from <a href=https://python.langchain.com/v0.1/docs/use_cases/tool_use//>Calling tools with Agents</a>\n",
    "    </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tools\n",
    "Resource: [Q&A with RAG -> More -> Using Agents](https://python.langchain.com/v0.1/docs/use_cases/question_answering/conversational_retrieval_agents/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_tool_calling_agent\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built in tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tool\n",
    "search = #\n",
    "\n",
    "# example\n",
    "search.run(\"How many children does Barack Obama have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tools\n",
    "tools = #\n",
    "\n",
    "# create llm with bound tools\n",
    "search_llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt\n",
    "search_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"\"\"You are a helpful assistant. You ALWAYS use the tools you have available instead of relying on internal information.\n",
    "     You are brief and succinct in your responses. You use tools first to find information.\n",
    "     You search the web to find answers about noteworthy figures or recent events.\"\"\"),\n",
    "    ('human', \"{text}\"),\n",
    "    ('placeholder', \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# create agent\n",
    "agent = # create the agent\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = # create the orchestrator of the overall excution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out\n",
    "response = agent_executor.invoke({'text':\"How many children does Barack Obama have?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tools\n",
    "tools = # add more tools\n",
    "\n",
    "# create agent\n",
    "agent = # create tool calling agent\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)\n",
    "\n",
    "# try it out\n",
    "response = agent_executor.invoke({'text':\"\"\"How many grand children would Barack Obama have if each of his children had 2 children?\n",
    "                                  I heard he recently had a baby!\"\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import SoupStrainer\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Creating custom retriever tools\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use website data\n",
    "sw_website = 'https://simple.wikipedia.org/wiki/Star_Wars_Episode_IV:_A_New_Hope'\n",
    "webloader = WebBaseLoader(sw_website,\n",
    "                       bs_kwargs = {'parse_only':SoupStrainer('div', id='bodyContent')})\n",
    "web_chunks = webloader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, add_start_index=True))\n",
    "print('Number of chunks generated: ', len(web_chunks))\n",
    "\n",
    "# create embeddings\n",
    "embeddings_fn = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") #, model_kwargs={\"device\":'mps'})\n",
    "hf_db = FAISS.from_documents(web_chunks, embeddings_fn)\n",
    "hf_retriever = hf_db.as_retriever(search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_name = \"Information Retriever for the Star Wars Movie: A New Hope\"\n",
    "description = \"\"\"You MUST use this tool whenever any information is requested about the Star Wars movie: A New Hope.\n",
    "    Make sure to rephrase the query so that it is not a question, but a statement to be compared with\n",
    "    text about the movie.\"\"\"\n",
    "\n",
    "# based off of the Guide we used\n",
    "retriever_tool = create_retriever_tool(\n",
    "    hf_retriever,\n",
    "    # add name\n",
    "    # add description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tools\n",
    "tools = # setup tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt\n",
    "react_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "    HumanMessagePromptTemplate.from_template(template='Answer the following questions as best you can or reply with the most meaningful ' +\n",
    "                                             'response possible. You have access to the following tools:\\n\\n{tools}\\n\\n although you can also reply conversationally when appropriate. '\n",
    "                                             'Use the following format to reason out your response:\\n\\n' +\n",
    "                                             'Question: the input question you must answer or statement to which you should reply\\n' +\n",
    "                                             'Thought: you should always think about what to do\\n' +\n",
    "                                             'Action: the action to take, should be one of [{tool_names}]\\n'+\n",
    "                                             'Action Input: the input to the action, making sure that the inputs are in the valid format for the action\\n'+\n",
    "                                             'Observation: the result of the action\\n... '+\n",
    "                                             '(this Thought/Action/Action Input/Observation can repeat up to 3 times)\\n'+\n",
    "                                             'Thought: I now know the final answer\\n'+\n",
    "                                             'Final Answer: the final answer to the original input question or appropriate response is\\n\\n'+\n",
    "                                             'Begin!\\n\\nQuestion or message: {input}\\nThought:{agent_scratchpad}'),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own llm and agent\n",
    "agent = # create another type of agent\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, run_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the agent\n",
    "agent_executor.invoke(\n",
    "    {\"input\": # add a question}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What did Obi-Wan do for Luke Skywalker in a New Hope?\n",
    "How did this influence affect the outcome of the Empire Strikes Back? You can use the web to find out more about other movies.\"\"\"\n",
    "\n",
    "# Execute the agent\n",
    "agent_executor.invoke(\n",
    "    {\"input\": #add a query}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the conversational component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with empty chat history. We can pass this as a parameter in the invoke\n",
    "\n",
    "\n",
    "# Create the agent with chat history\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "            # add required agent\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    handle_parsing_errors = \"Check your output and make sure it conforms, use the Action/Action Input syntax\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start chatting away\n",
    "query_1 = \"\"\"What did Obi-Wan do for Luke Skywalker in a New Hope?\n",
    "How did this influence affect the outcome of the Empire Strikes Back? You can use the web to find out more about other movies.\"\"\"\n",
    "\n",
    "session_id_1 = \"session_1\"\n",
    "\n",
    "# Execute the agent\n",
    "agent_with_chat_history.invoke({\"input\":# add query}, config={\"configurable\": {\"session_id\": session_id_1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start chatting away\n",
    "query_2 = \"\"\"Elaborate on your previous answer, and provide details that reference the object that Obi-Wan gave to Luke Skywalker in A New Hope.\"\"\"\n",
    "\n",
    "# Execute the agent\n",
    "agent_with_chat_history.invoke({\"input\": # follow up conversation}, config={\"configurable\": {\"session_id\": session_id_1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing it all together\n",
    "With a user history, a tool, and a retriever, you can now create a conversational agent through a quick UI that can answer questions and provide information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history.clear()\n",
    "session_id_chat = 'session_id_chat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create response\n",
    "def ai_response(user_message, chat_info):\n",
    "    agent_message = {'input':user_message}\n",
    "    response = agent_with_chat_history.invoke(agent_message, config={\"configurable\": {\"session_id\": session_id_chat}})\n",
    "    conversation = message_history.messages\n",
    "    formatted_messages = [(conversation[ind].content, conversation[ind+1].content)\n",
    "                           for ind in range(0, len(conversation), 2)]\n",
    "    return '', formatted_messages\n",
    "\n",
    "# create gradio blocks UI\n",
    "with gr.Blocks() as demo :\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    msg.submit(ai_response, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Assistants - Agents! (?)\n",
    "Resource: [Assistants API](https://platform.openai.com/docs/assistants/overview)\n",
    "\n",
    "Resource: [Platform Documentation - time for the playground!](https://platform.openai.com/playground/assistants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create assistant with code interpreter\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    tools=[{\"type\": \"code_interpreter\", \"type\": \"file_search\"}],\n",
    "    model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EASY message history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a thread of message history\n",
    "thread = client.beta.threads.create()\n",
    "thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a message to the thread\n",
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    ")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create_and_poll(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"The user prefers to have concepts explained to them like they're 5. Then, describe it at an appropriate college level.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run.status == 'completed': \n",
    "  messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "  )\n",
    "  print('Finished running.')\n",
    "else:\n",
    "  print(run.status)\n",
    "\n",
    "messages.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get message information\n",
    "print(len(messages.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the ai response\n",
    "print(messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all messages\n",
    "for ind, message in enumerate(messages.data):\n",
    "    print(f'***** Message {ind}:\\n{message.content[0].text.value}\\n *****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view entire run information\n",
    "run.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools: Code Interpreter\n",
    "Resource: [Code Interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -o palmer_penguins.csv https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the penguins file to be used directly with a message thread with code interpreter\n",
    "palmer_penguins_data = client.files.create(\n",
    "    file=open(\"palmer_penguins.csv\", \"rb\"),\n",
    "    purpose='assistants'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify thread to include the penguins file\n",
    "ci_thread = client.beta.threads.create(\n",
    "    tool_resources={\"code_interpreter\": {\"file_ids\": [palmer_penguins_data.id]}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new message to call palmer penguins\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=ci_thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to analyze the Palmer Penguins data I uploaded. Create a histogram reflecting the counts of the different species of penguins.\",\n",
    "    attachments=[\n",
    "        {\n",
    "            \"file_id\":palmer_penguins_data.id,\n",
    "            \"tools\":[{\"type\":\"code_interpreter\"}]\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create run and force use of code interpreter\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "  thread_id=ci_thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  tools=[{\"type\":\"code_interpreter\"}],\n",
    "  tool_choice = {\"type\":\"code_interpreter\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check completion and view output\n",
    "if run.status == 'completed': \n",
    "  messages = client.beta.threads.messages.list(\n",
    "    thread_id=ci_thread.id\n",
    "  )\n",
    "  print('Finished running.')\n",
    "else:\n",
    "  print(run.status)\n",
    "\n",
    "messages.model_dump()\n",
    "#run.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(messages.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve a single image file\n",
    "#generated_image = messages.data[0].content[0].image_file.file_id\n",
    "#print(generated_image)\n",
    "\n",
    "# download using file api\n",
    "#image_data = client.files.content(generated_image)\n",
    "#image_data_bytes = image_data.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Vector Stores\n",
    "Resource: [File Search](https://platform.openai.com/docs/assistants/tools/file-search/file-search-beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -o declaration_independence.pdf https://www.uscis.gov/sites/default/files/document/guides/M-654.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store caled \"Financial Statements\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"declaration of independence\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\"declaration_independence.pdf\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create assistant with code interpreter\n",
    "history_assistant = client.beta.assistants.create(\n",
    "    name=\"Prominent H. Figure\",\n",
    "    instructions=\"You are a prominent historical figure from the late 1770s. You're an opinionated dynamic oratorical speaker, but you make your point succinctly!\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_assistant = client.beta.assistants.update(\n",
    "  assistant_id=history_assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create thread for history\n",
    "history_thread = client.beta.threads.create()\n",
    "history_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and send message\n",
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=history_thread.id,\n",
    "  role=\"user\",\n",
    "  content=\"What reasons do the authors cite for declaring independence from Great Britain in the Declaration of Independence?\"\n",
    ")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create run and force use of code interpreter\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "  thread_id=history_thread.id,\n",
    "  assistant_id=history_assistant.id,\n",
    ")\n",
    "\n",
    "if run.status == 'completed': \n",
    "  messages = client.beta.threads.messages.list(\n",
    "    thread_id=history_thread.id\n",
    "  )\n",
    "  print('Finished running.')\n",
    "else:\n",
    "  print(run.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view result\n",
    "messages.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!! \n",
    "You have now learned how to use tools, agents, and assistants to enhance the capabilities of your generative AI applications. You have experience using the OpenAI Completions and Assistants API, are able to execute RAG with conversation history, and are able to port this knowledge to also using agents and tools.\n",
    "\n",
    "Next week, we'll start model training - congratulations again!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
